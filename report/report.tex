\documentclass[11pt,a4paper]{article}

% ─── Packages ──────────────────────────────────────────────────────────────
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{microtype}

% ─── Theorem environments ──────────────────────────────────────────────────
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}

% ─── Shortcuts ─────────────────────────────────────────────────────────────
\newcommand{\NFA}{\mathrm{NFA}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{arg\,min}

% ─── Title ─────────────────────────────────────────────────────────────────
\title{%
  A-Contrario Homography Registration with ORSA:\\
  Implementation, Analysis, and Experiments%
}
\author{
  [Name]\\
  MVA, ENS Paris-Saclay\\
  \texttt{[email]}
}
\date{February 2025}

% ═══════════════════════════════════════════════════════════════════════════
\begin{document}
\maketitle

\begin{abstract}
We present a complete implementation and experimental analysis of the ORSA
(Optimized Random Sampling) algorithm for automatic homographic registration
of image pairs, following the a-contrario framework of Moisan, Moulon, and
Monasse~\cite{moisan2012}. Unlike classical RANSAC, which requires a
manually tuned inlier threshold, ORSA adaptively selects the threshold that
minimizes the Number of False Alarms (NFA), providing rigorous statistical
control on false detections. We describe the detection problem, derive the
NFA formula, verify it satisfies the theoretical requirements, detail our
implementation, and present experiments on synthetic and real data covering
null-model validation, robustness to outliers, sensitivity analysis, and
failure cases.
\end{abstract}

% ═══════════════════════════════════════════════════════════════════════════
\section{Introduction}\label{sec:intro}

\subsection{Detection problem}

\subsection{A-contrario viewpoint and Gestalt link}

The a-contrario framework draws its motivation from the Helmholtz
principle~\cite{desolneux2000,desolneux2008}: a structure is perceptually
meaningful if it is very unlikely to arise by chance. When a set of point
correspondences all agree with a single homography, this agreement seems
``too organized to be accidental.'' The a-contrario approach makes this
intuition precise. It defines a null model describing random, unstructured
data, and then asks: how surprising would the observed agreement be if the
data were truly random? The Number of False Alarms (NFA) quantifies this
surprise. A detection is declared when $\NFA < 1$, which means the
observed structure would be expected to occur less than once in purely
random data.

\subsection{Contributions}

We implement the full ORSA pipeline following~\cite{moisan2012} and
evaluate it on both synthetic and real data. On the theoretical side,
we provide a formal proof that the NFA satisfies the a-contrario
guarantee, and we validate this property empirically by running ORSA
on purely random data. On the experimental side, we measure robustness
against outlier ratios ranging from 10\% to 90\%, test sensitivity to
the iteration budget and to random seed choices, and analyze failure
cases such as extreme outlier ratios and scenes with multiple
homographies. We also present results on real image pairs with
qualitative visualizations. The implementation handles degeneracy
checks, log-space NFA computation, and iterative refinement with
Levenberg-Marquardt polishing.

% ═══════════════════════════════════════════════════════════════════════════
\section{A-Contrario Model and NFA for Homography}\label{sec:nfa}

\subsection{Background model}\label{sec:background}

\subsection{Symmetric transfer error}\label{sec:residual}

\subsection{Event, precision, and family of tests}

\paragraph{Event.}

\paragraph{Precision parameter.}

\paragraph{Family of tests.}

\paragraph{Number of tests.}

\subsection{NFA formula and log-space computation}

% ═══════════════════════════════════════════════════════════════════════════
\section{Full Derivation: It Is Truly an NFA}\label{sec:verification}

\subsection{Null model}

\subsection{Precise event being tested}

\subsection{Probability bound under the null}

% ═══════════════════════════════════════════════════════════════════════════
\section{The Algorithm}\label{sec:algorithm}

\subsection{ORSA loop}
The ORSA algorithm (\Cref{alg:orsa}) follows the same general strategy
as RANSAC---repeatedly drawing random samples and fitting candidate
models---but replaces the fixed inlier threshold with the a-contrario
NFA criterion. At each iteration, a candidate homography is estimated
from 4 random correspondences, and the NFA is evaluated at every
possible inlier count. The best model across all iterations is the one
that minimizes the NFA.

\begin{algorithm}[htbp]
\caption{ORSA for homography estimation}\label{alg:orsa}
\begin{algorithmic}[1]
\Require $n$ correspondences $(x_i, x'_i)$, image dimensions, max iterations $T$
\Ensure Best homography $H^*$, inlier mask, $\NFA^*$
\State Precompute $\log_{10}\binom{n}{k}$ for $k=0,\ldots,n$ and
       $\log_{10}\binom{m}{4}$ for $m=0,\ldots,n$
\State $\NFA^* \gets +\infty$, $H^* \gets \texttt{None}$
\For{$t = 1, \ldots, T$}
  \State Draw 4 correspondences uniformly at random \label{line:sample}
  \State Check degeneracy (collinearity, conditioning, orientation on sample)
  \If{degenerate} \textbf{continue} \EndIf
  \State Estimate $H$ by DLT with Hartley normalization
  \State Check valid warp (corners map to reasonable locations)
  \State Compute residuals $e_i$ and side indicators $s_i$ for all $n$ matches
  \State Sort residuals: $e_{\sigma(1)} \leq \cdots \leq e_{\sigma(n)}$
  \For{$k = 5, \ldots, n$}
    \State $r_k^2 \gets e_{\sigma(k)}$,\quad
           $\alpha_k \gets \pi\,r_k^2/(w_{s_{\sigma(k)}}h_{s_{\sigma(k)}})$
    \State Compute $\log_{10}\NFA(k)$ via \eqref{eq:lognfa}
  \EndFor
  \State $k^* \gets \argmin_k \NFA(k)$
  \If{$\NFA(k^*) < \NFA^*$}
    \State Update $H^*$, inlier mask, $\NFA^*$
    \State Update adaptive iteration count based on inlier ratio
  \EndIf
\EndFor
\State \textbf{Refinement:} Iteratively refit $H^*$ on inliers (DLT),
       recompute NFA, until convergence
\State \textbf{Polish:} Levenberg-Marquardt optimization on inlier
       correspondences
\State \Return $H^*$, inlier mask, $\NFA^*$
\end{algorithmic}
\end{algorithm}

\subsection{Degeneracy handling}\label{sec:degeneracy}
Not every 4-point sample produces a usable homography. Several checks
are applied to reject degenerate or physically implausible candidates
before they can affect the NFA scoring.

First, we check for collinearity: if any 3 of the 4 sampled points are
nearly collinear (triangle area below a threshold), the sample is
discarded, since the resulting homography would be poorly constrained.
Second, we look at the condition number of the normalized $H$ matrix. A
condition number above~10 indicates numerical instability, so such
candidates are rejected. This check is performed before denormalization
in the DLT, following the IPOL reference~\cite{moisan2012}.

Third, we verify that the homography preserves orientation. Concretely,
we check that $w' = H_{31}x + H_{32}y + H_{33} > 0$ at the sample
points. This is only checked on the 4 sample points, not on all
correspondences, because outlier points may legitimately violate this
condition. Any correspondence where $w' \leq 0$ is assigned infinite
error in the symmetric transfer error computation. Finally, a valid
warp check ensures that the four image corners map to finite, reasonable
locations, with the output-to-input area ratio staying within
$[1/100, 100]$.

\subsection{Refinement}\label{sec:refinement}
The homography returned by the ORSA loop is estimated from only 4
points, so it can usually be improved. We apply two refinement stages.

The first is an iterative DLT refit: we re-estimate the homography
using all current inliers (not just the original 4 sample points),
recompute the residuals and the NFA, and update the inlier set
accordingly. This process is repeated until the NFA no longer improves,
which typically takes 2 to 5 iterations.

The second stage is a Levenberg-Marquardt optimization. Starting from
the refined DLT solution, we minimize the forward transfer error over
the inlier correspondences, treating the 8 independent entries of $H$
as free parameters ($H_{33}$ is fixed to~1). The result is accepted
only if it does not worsen the NFA.

\subsection{Computational complexity}
Each iteration of the main loop performs a fixed-size DLT estimation in
$O(1)$, computes all $n$ residuals in $O(n)$, sorts them in
$O(n \log n)$, and evaluates the NFA for every possible inlier count in
$O(n)$ thanks to the precomputed binomial coefficient tables. The
sorting step dominates, so the cost per iteration is $O(n \log n)$.
Over $T$ iterations, the total complexity is $O(Tn\log n)$. In
practice, the adaptive stopping rule often terminates well before the
maximum number of iterations when the inlier ratio is high, since a
good model is found early.

% ═══════════════════════════════════════════════════════════════════════════
\section{Experiments}\label{sec:experiments}

We conduct a series of experiments to validate the implementation and
understand how the method behaves under different conditions.

\subsection{Null model validation}\label{sec:exp_null}

The most basic sanity check for an a-contrario method is to verify that
it does not detect anything when there is nothing to detect. To test
this, we generate purely random correspondences with no underlying
homography, using $n \in \{50, 100, 200, 500\}$ matches. For each
value of $n$, we run 50 independent trials with 500 ORSA iterations
each.

\Cref{tab:null_model} shows the results. Out of 200 total trials, only
one produced a (marginal) false detection ($\log_{10}\NFA = -0.67$ at
$n=50$). All other trials returned $\log_{10}\NFA = 0$, meaning no
model was deemed significant.

\begin{table}[htbp]
\centering
\caption{Null model validation: false alarm rate on pure random data.}
\label{tab:null_model}
\begin{tabular}{@{}lcccc@{}}
\toprule
$n$ (outliers) & 50 & 100 & 200 & 500 \\
\midrule
False alarms / 50 trials & 1 & 0 & 0 & 0 \\
Mean $\log_{10}\NFA$ & $-0.01$ & $0.0$ & $0.0$ & $0.0$ \\
Min $\log_{10}\NFA$ & $-0.67$ & $0.0$ & $0.0$ & $0.0$ \\
\bottomrule
\end{tabular}
\end{table}

When no candidate model achieves $\NFA < 1$, the algorithm reports
$\log_{10}\NFA = 0$ (i.e.\ $\NFA = 1$), meaning ``no detection.'' This
explains the strong concentration at~0 in \Cref{fig:null_model}. The
result is consistent with the theoretical guarantee (\Cref{thm:nfa}):
the expected number of false alarms under the null model is at most~1,
and our observed false alarm rate of 0.5\% is well within this bound.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/null_model.png}
  \caption{Distribution of $\log_{10}\NFA$ under the null model.
  Nearly all values are at 0 (no detection), validating the
  a-contrario false alarm control.}
  \label{fig:null_model}
\end{figure}

\subsection{Synthetic controlled experiments}\label{sec:exp_synthetic}

We generate synthetic data from a known mild-perspective homography
$H_{\mathrm{true}}$, with $n=200$ total correspondences and Gaussian
noise of $\sigma=1$ pixel on the inlier projections. The outlier ratio
varies from 10\% to 90\%, and we run 10 trials per condition.

\Cref{tab:synthetic} summarizes the results. Precision and recall are
reported \emph{conditional on detection}: they are averaged only over
trials where ORSA found a meaningful model ($\NFA < 1$). Trials where
no model was detected contribute to the detection rate column but not
to precision or recall. ORSA achieves perfect precision (no false
inliers) at all outlier ratios, and a 100\% detection rate up to 70\%
outliers. At 90\% outliers, only 20 of the 200 correspondences are
true inliers, and the probability of drawing 4 inliers at random
becomes very low. Accordingly, detection drops to 30\%.

\begin{table}[htbp]
\centering
\caption{Synthetic experiments: performance vs.\ outlier ratio.}
\label{tab:synthetic}
\begin{tabular}{@{}lccccc@{}}
\toprule
Outlier ratio & 10\% & 30\% & 50\% & 70\% & 90\% \\
\midrule
Detection rate & 10/10 & 10/10 & 10/10 & 10/10 & 3/10 \\
Precision & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
Recall & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 \\
Mean $\log_{10}\NFA$ & $-666$ & $-486$ & $-318$ & $-172$ & $-40$ \\
Corner error (px) & 0.44 & 0.41 & 0.48 & 0.77 & 1.03 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/simple_synthetic.png}
  \caption{Left: precision and recall vs.\ outlier ratio.
  Right: homography accuracy (corner error) vs.\ outlier ratio.}
  \label{fig:synthetic}
\end{figure}

\subsection{Real images}\label{sec:exp_real}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{figures/real_easy_imgA.png}
    \caption{Easy case (imgA): 564/1254 inliers, $\log_{10}\NFA=-1645$.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{figures/real_hard_imgB.png}
    \caption{Hard case (imgB): 1023/1212 inliers, $\log_{10}\NFA=-3991$.}
  \end{subfigure}
  \caption{ORSA results on real image pairs. Green lines: inliers; red
  lines: outliers. Top-left: match visualization; top-right: warped blend;
  bottom-left: NFA curve; bottom-right: error histogram.}
  \label{fig:example_matches}
\end{figure}

\paragraph{Easy case (imgA).}
The first pair consists of two bookshelf photographs with a mild
perspective change, yielding 1254 SIFT matches. ORSA identifies 564
inliers (45\%) with $\log_{10}\NFA = -1645$ and a threshold of
$\varepsilon = 30.3$\,px (\Cref{fig:example_matches}a). The mean
reprojection error is 13.3\,px, and the computation takes 51\,ms. The
extremely negative NFA indicates a very strong detection. The
relatively large threshold reflects the high resolution of the images:
at higher resolution, even accurately matched points can be several
pixels apart.

\paragraph{Hard case (imgB).}
The second pair shows two building photographs with a more pronounced
perspective change. Out of 1212 SIFT matches, ORSA finds 1023 inliers
(84\%) with $\log_{10}\NFA = -3991$ and a much tighter threshold of
$\varepsilon = 4.1$\,px (\Cref{fig:example_matches}b). The mean
reprojection error is only 1.6\,px. The smaller threshold compared to
imgA reflects less geometric distortion between the two views, and the
high inlier ratio shows that most matches in this pair are genuine.

\subsection{Sensitivity analysis}\label{sec:exp_sensitivity}

\paragraph{Iteration budget.}
We vary the maximum number of ORSA iterations to see how many are
actually needed. \Cref{tab:sensitivity_iter} shows that results
converge by about 500 iterations. Increasing the budget further to
5000 produces no improvement in either the log-NFA or the corner
error, which confirms that the adaptive stopping rule works well.

\begin{table}[htbp]
\centering
\caption{Sensitivity to iteration budget (50\% outliers, $n=200$).}
\label{tab:sensitivity_iter}
\begin{tabular}{@{}lccccc@{}}
\toprule
max\_iter & 50 & 100 & 500 & 1000 & 5000 \\
\midrule
Mean $\log_{10}\NFA$ & $-316$ & $-316$ & $-316$ & $-316$ & $-316$ \\
Corner error (px) & 0.61 & 0.57 & 0.58 & 0.58 & 0.58 \\
Runtime (ms) & 17 & 21 & 22 & 23 & 23 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Seed stability.}
Since ORSA relies on random sampling, different random seeds could in
principle produce different results. We run 20 trials on the same data
with different seeds. The $\log_{10}\NFA$ varies by only 3.4 units
(from $-315$ to $-319$), and the inlier count varies by at most 5
(between 95 and 100 out of 100 true inliers). This shows that the
algorithm is very stable with respect to randomness.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/sensitivity.png}
  \caption{Left: corner error vs.\ max iterations. Right: NFA stability
  across 20 random seeds.}
  \label{fig:sensitivity}
\end{figure}

\subsection{Failure cases}\label{sec:exp_failure}
Understanding where the algorithm fails is as important as showing where
it succeeds. We test three challenging scenarios.

\paragraph{Pure random matches (300 points).}
With 300 purely random correspondences and no underlying homography,
ORSA correctly returns no detection ($\log_{10}\NFA = 0$). This is a
larger-scale version of the null-model experiment and confirms that the
false alarm control holds even with more data.

\paragraph{Multi-structure (two conflicting homographies).}
We generate two groups of 80 inliers, each consistent with a different
homography, plus 50 random outliers (195 correspondences in total).
ORSA finds the dominant structure (76 inliers, $\log_{10}\NFA = -196$)
and treats the second group's inliers as outliers. This is the expected
behavior for a single-model method: ORSA looks for one homography, not
two. Recovering both structures would require applying ORSA iteratively,
removing detected inliers between runs (the exclusion principle).

\paragraph{Extreme outlier ratio (5 inliers / 300 outliers).}
With only 5 true inliers among 305 correspondences (98.4\% outliers),
the probability of randomly drawing 4 inliers in a single sample is
$\binom{5}{4}/\binom{305}{4} \approx 7\times10^{-9}$. Over 20 trials,
zero detections occur. This is an expected and unavoidable failure: no
sampling-based method can work when clean samples are this unlikely.

% ═══════════════════════════════════════════════════════════════════════════
\section{Implementation Details}\label{sec:implementation}

The codebase is split into focused modules, each handling one aspect of
the pipeline. The NFA computation (\texttt{src/nfa.py}) works entirely
in log-space to avoid numerical overflow, and relies on precomputed
binomial coefficient tables. It uses the formula
$\binom{n}{k}\binom{k}{4}$ from the paper, which we verified is
distinct from the alternative $\binom{n-4}{k-4}\binom{k}{4}$ sometimes
seen in other implementations. Homography estimation
(\texttt{src/homography.py}) provides DLT with Hartley normalization,
the symmetric transfer error, and Levenberg-Marquardt refinement. All
degeneracy checks described in \Cref{sec:degeneracy} are implemented in
\texttt{src/degeneracy.py}. The main ORSA loop (\texttt{src/orsa.py})
ties everything together, with adaptive stopping and iterative
refinement. Feature matching (\texttt{src/matching.py}) supports both
SIFT and ORB detectors with Lowe's ratio test for filtering. The
\texttt{experiments/} directory contains the synthetic data generation
code and scripts for the 6 experiment types presented in this report.
The full test suite (\texttt{tests/}) contains 46 unit tests covering
all modules.

\paragraph{Key design decisions.}
One subtle but important point concerns the error multiplier in
log-space. Since $\alpha = \pi r^2/(wh)$ and our residuals $e_i$ are
already squared distances ($e_i = r^2$), we have
$\log_{10}\alpha = \log_{10}(\pi/(wh)) + 1.0 \cdot \log_{10}(e)$. A
multiplier of $0.5$ would implicitly take the square root of the error,
giving the wrong dimensionality. We also cap all errors at the squared
image diagonal, which prevents correspondences with absurdly large
residuals from being counted as inliers at any threshold. Finally,
throughout the ORSA loop, we track the best model seen so far (the one
with the lowest NFA, even if that NFA is above~1). This allows the
focused sampling heuristic to work from the first iteration, while a
detection is only declared when $\NFA < 1$.

\paragraph{Use of AI tools.}
AI tools (Claude) were used for code structure and glue code, debugging
and code review (identifying discrepancies between the implementation and
the paper's formulas), and report formatting. The core algorithmic logic
was implemented based on direct reading of the IPOL
paper~\cite{moisan2012}, and all formulas and proofs were verified manually
against the course material.

% ═══════════════════════════════════════════════════════════════════════════
\section{Discussion}\label{sec:discussion}

\subsection{Strengths of the framework}

\subsection{Limitations}

\paragraph{Extreme outlier ratios.}
ORSA, like all RANSAC-based methods, relies on randomly drawing a
sample of 4 inliers. When the outlier ratio exceeds roughly 90\%, the
probability of drawing such a clean sample becomes vanishingly small:
with $k$ inliers among $n$ correspondences, this probability is
$\binom{k}{4}/\binom{n}{4}$, which drops off sharply as $k/n$
decreases. Our experiments confirm that detection fails consistently at
98\% outliers (5 inliers among 300).

\paragraph{Single homography model.}
ORSA is designed to find a single homography. When a scene contains
multiple planes (e.g.\ a building with several facades), each plane
defines a different homography, and these structures compete with one
another. As shown in \Cref{sec:exp_failure}, ORSA recovers the
dominant one and treats the rest as outliers. Detecting multiple
homographies would require applying ORSA iteratively, removing detected
inliers between runs.

\paragraph{Very few inliers.}
Even when the outlier ratio is moderate, a small absolute number of
inliers (e.g.\ fewer than 10) makes detection difficult. The
combinatorial terms $\binom{n}{k}\binom{k}{4}$ in the NFA formula grow
rapidly with $n$ and penalize small values of $k$ heavily. In other
words, the NFA is inherently conservative: it requires enough
statistical evidence to overcome the correction for multiple testing.

\subsection{Interpretability and meaning of the NFA value}

\subsection{NFA is a detection criterion, not an accuracy metric}

\subsection{Model bias from the null hypothesis}

The a-contrario framework tests against a specific null model: the
correspondences are assumed to be independent, and each target point is
assumed to be uniformly distributed over the image. This is a reasonable
approximation for putative matches produced by a feature detector, but
real data can deviate from it in several ways.

For instance, keypoints are often spatially clustered in textured
regions rather than spread uniformly, which can make detections appear
more surprising than they truly are. Conversely, repetitive textures
can produce correlated false matches that violate the independence
assumption, which could in principle lead to false detections. The
probability bound $\alpha(r) = \pi r^2/(wh)$ also ignores boundary
effects: for points near the image edges, the true probability of a
random match falling within distance $r$ may be lower than the disk
approximation suggests, making the NFA slightly conservative.

Despite these simplifications, the null model works well in practice.
The key reason is that the bound is conservative by design: when the
true probability is lower than $\alpha(r)$, the NFA is overestimated,
which only makes it harder to trigger a false alarm.

% ═══════════════════════════════════════════════════════════════════════════
\section{Conclusion}\label{sec:conclusion}

We presented a complete implementation of the ORSA algorithm for
a-contrario homography registration. Our experiments confirm the main
theoretical guarantee: the false alarm rate on random data is near
zero, consistent with the bound of at most one expected false detection.
On synthetic data, ORSA detects the correct homography with perfect
precision up to 70\% outliers, and degrades gracefully beyond that
point. The results are stable across random seeds and iteration
budgets. Overall, the a-contrario framework provides a principled,
parameter-free alternative to fixed-threshold RANSAC. The NFA plays a
dual role: it serves as the model selection criterion during the
algorithm, and as a measure of how meaningful the final detection is.

% ═══════════════════════════════════════════════════════════════════════════
\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Moisan et~al.(2012)]{moisan2012}
L.~Moisan, P.~Moulon, and P.~Monasse.
\newblock Automatic homographic registration of a pair of images, with a
  contrario elimination of outliers.
\newblock \emph{Image Processing On Line}, 2:56--73, 2012.

\bibitem[Desolneux et~al.(2000)]{desolneux2000}
A.~Desolneux, L.~Moisan, and J.-M. Morel.
\newblock Meaningful alignments.
\newblock \emph{International Journal of Computer Vision}, 40(1):7--23, 2000.

\bibitem[Desolneux et~al.(2008)]{desolneux2008}
A.~Desolneux, L.~Moisan, and J.-M. Morel.
\newblock \emph{From Gestalt Theory to Image Analysis: A Probabilistic
  Approach}.
\newblock Springer, 2008.

\bibitem[Lowe(2004)]{lowe2004}
D.~G. Lowe.
\newblock Distinctive image features from scale-invariant keypoints.
\newblock \emph{International Journal of Computer Vision}, 60(2):91--110, 2004.

\bibitem[Fischler and Bolles(1981)]{fischler1981}
M.~A. Fischler and R.~C. Bolles.
\newblock Random sample consensus: a paradigm for model fitting with
  applications to image analysis and automated cartography.
\newblock \emph{Communications of the ACM}, 24(6):381--395, 1981.

\bibitem[Hartley and Zisserman(2003)]{hartley2003}
R.~Hartley and A.~Zisserman.
\newblock \emph{Multiple View Geometry in Computer Vision}.
\newblock Cambridge University Press, 2nd edition, 2003.

\bibitem[Chum and Matas(2005)]{chum2005}
O.~Chum and J.~Matas.
\newblock Matching with {PROSAC}---progressive sample consensus.
\newblock In \emph{IEEE CVPR}, pages 220--226, 2005.

\bibitem[Chum et~al.(2003)]{chum2003}
O.~Chum, J.~Matas, and J.~Kittler.
\newblock Locally optimized {RANSAC}.
\newblock In \emph{DAGM}, pages 236--243, 2003.

\end{thebibliography}

\end{document}
